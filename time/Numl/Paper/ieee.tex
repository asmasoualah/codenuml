
\section{Floating-Point Arithmetic}
\label{ieee}

In this section we introduce some preliminary notions helpful to understand the rest of the article. Elements of floating-point arithmetic are introduced in Section \ref{arith}.
Further, an illustration of what our method does is given in Section \ref{run}.
Related work is discussed in Section \ref{relate}.

\subsection{Elements of Floating-Point Arithmetic}
\label{arith}

We introduce here some elements of floating-point arithmetic \cite{IEEE754,Mul10}. First of all, a
{\it floating-point number} $x$ in base $\beta$ is defined by
\begin{equation}
\label{fpnum}\small
x = s \cdot (d_0.d_1\ldots d_{p-1}) \cdot \beta^e = s\cdot m \cdot \beta^{e-p+1}
\end{equation}
where 
%\begin{itemize}
 $s\in \{-1,1\}$ is the sign, 
$m=d_0d_1\ldots d_{p-1}$ is the
{\it significand}, $0\le d_i< \beta$, $0\le i\le p-1$, 
 $p$ is the
{\it precision} and  $e$ is the exponent, $e_{min}\le e \le e_{max}$.

A floating-point number $x$ is {\it normalized} whenever $d_0\not= 0$.  Normalization
avoids multiple representations of the same number.
The IEEE754 Standard also defines denormalized numbers which are floating-point
numbers with $d_0=d_1=\ldots=d_k=0$, $k< p-1$ and $e=e_{min}$.  Denormalized numbers make underflow gradual \cite{Mul10}.
The IEEE754 Standard defines binary formats (with $\beta=2$) and decimal formats (with $\beta=10$). In this article,
without loss of generality, we only consider normalized numbers and we always assume that $\beta=2$ (which is the most common case in practice).
The IEEE754 Standard also specifies a few
values for $p$, $e_{min}$ and $e_{max}$ which are summarized in Figure \ref{formats}.
Finally,  special values also are defined:
%\begin{itemize} 
\textsf{nan}\ (Not a Number) resulting from an invalid operation,
 $\pm\infty$ corresponding to overflows,
and $+0$ and $-0$ (signed zeros).
%\end{itemize}

\begin{figure}[tb]
\scriptsize
\hrule
\vspace{0.2cm}
\centerline{
\begin{tabular}{cccccccccccccccccc}
&Format    &&& Name &&& $p$ &&& $e$ bits  &&& $e_{min}$ &&& $e_{max}$ & \\
\hline
&Binary16  &&& Half precision            &&& 11  &&& 5              &&& $-14 $  &&& $+15$ &     \\
&Binary32  &&& Single precision            &&& 24  &&& 8              &&& $-126 $  &&& $+127$ &     \\
&Binary64  &&& Double precision           &&& 53  &&& 11             &&& $-1122 $  &&& $+1223$&     \\
\hspace{0.2cm} & Binary128 &  \hspace{0.2cm}  &  \hspace{0.2cm}  & Quadruple precision            & \hspace{0.2cm}  &  \hspace{0.2cm}  &113  & \hspace{0.2cm} &  \hspace{0.2cm} & 15             & \hspace{0.2cm}  & \hspace{0.2cm} & $-16382 $  & \hspace{0.2cm}  & \hspace{0.2cm} & $+16383$ &  \hspace{0.2cm}      \\
\end{tabular}}\vspace{0.2cm}
\hrule
\normalsize
\caption{\label{formats}Basic binary IEEE754  formats.}
\end{figure}
The IEEE754 Standard also defines five rounding modes for elementary operations over
floating-point numbers. These modes are towards $-\infty$, towards $+\infty$, towards zero,
 to the nearest ties to even and to the nearest ties to away and we write them $\circ_{-\infty}$, $\circ_{+\infty}$, $\circ_0$, 
$\circ_{\sim_e}$ and $\circ_{\sim_a}$, respectively.
The semantics of the elementary operations $\diamond\in\{+,\ -,\ \times,\ \div\}$  is then defined by
\begin{equation}
\label{roundoff}\small
f_1\ \diamond_{\circ}\ f_2\ =\ \circ(f_1\ \diamond\ f_2) 
\end{equation}
%
where $\circ\in\{\circ_{-\infty},\circ_{+\infty},\circ_{0},\circ_{\sim_e},\circ_{\sim_a}\}$ denotes the rounding mode.
Equation (\ref{roundoff}) states that the result of a floating-point operation $\diamond_\circ$ done
with the rounding mode $\circ$ returns what we would obtain by performing the exact operation $\diamond$
and next rounding the result using $\circ$.
The IEEE754 Standard also specifies how the square root function must be rounded in a similar way
to Equation (\ref{roundoff}) but does not specify
the roundoff of other functions like sin, log, etc.

We introduce hereafter two functions which compute the  \textit{u}nit in the \textit{f}irst \textit{p}lace
and  the \textit{u}nit in the \textit{l}ast \textit{p}lace
of a floating-point number. These functions are used further in this article to generate constraints encoding the way
roundoff errors are propagated throughout computations.
The $\mathsf{ufp}$ of a number $x$ is 
\begin{equation}\small\label{equfp}
\mathsf{ufp}(x) = \min \big\{ i\in \mathbb{N}\ :\ 2^{i+1} > x\big\} = \lfloor \log_2(x)\rfloor\enspace.
\end{equation}
The $\mathsf{ulp}$ of a floating-point number which significand has size $p$ is defined by
\begin{equation}\small
 \mathsf{ulp}(x) = \mathsf{ufp}(x) - p+1\enspace .
\end{equation}
The $\mathsf{ufp}$ of a floating-point number corresponds to the binary exponent of its most significant digit.
Conversely, the $\mathsf{ulp}$ of a floating-point number corresponds to the binary exponent of its least significant digit.
Note that several definitions of the $\mathsf{ulp}$ have been given \cite{Mul05}.

\begin{figure}[t]
\begin{center}\tt\scriptsize
\hrule
\vspace{0.2cm}
\begin{tabular}{lll}
\begin{lstlisting}[mathescape]
x$_{t-1}$:=[1.0,3.0]#16;
x$_t$:=[1.0,3.0]#16;
y$_{t-1}$:=0.0;
while(c) {
  u:=0.3 * y$_{t-1}$; 
  v:=0.7 * (x$_t$ + x$_{t-1}$);
  y$_t$:=u + v;
  y$_{t-1}$:=y$_t$;
};
require_accuracy(y$_t$,10);
  \end{lstlisting}
& &


  \begin{lstlisting}[mathescape]
x$_{t-1}\preci{9}$:=[1.0,3.0]$\preci{9}$; x$_t\preci{9}$:=[1.0,3.0]$\preci{9}$;
y$_{t-1}\preci{10}$:=0.0$\preci{10}$;
while(c) {
  u$\preci{10}$:=0.3$\preci{10}$ *$\preci{10} $ y$_{t-1}\preci{10}$; 
  v$\preci{10}$:=0.7$\preci{11}$ *$\preci{10}$ (x$_t\preci{9}$ +$\preci{10}$ x$_{t-1}\preci{9}$);
  y$_t\preci{10}$:=u$\preci{10}$ +$\preci{10}$ v$\preci{10}$;
  y$_{t-1}\preci{10}$:=y$_t\preci{10}$; };
require_accuracy(y$_t$,10);
  \end{lstlisting}
\\
&&
\\

  \begin{lstlisting}[mathescape]
x$_{t-1}\preci{16}$:=[1.0,3.0]$\preci{16}$;
x$_t\preci{16}$:=[1.0,3.0]$\preci{16}$;
y$_{t-1}\preci{52}$:=0.0$\preci{52}$;
u$\preci{52}$:=0.3$\preci{52}$ *$\preci{52} $ y$_{t-1}\preci{52}$; 
v$\preci{15}$:=0.7$\preci{52}$ *$\preci{15}$ (x$_t\preci{16}$ +$\preci{16}$ x$_{t-1}\preci{16}$);
y$_t\preci{15}$:=u$\preci{52}$ +$\preci{15}$ v$\preci{15}$;
y$_{t-1}\preci{15}$:=y$_t\preci{15}$;
  \end{lstlisting}
&&

  \begin{lstlisting}[mathescape]
x$_{t-1}\preci{9}$:=[1.0,3.0]$\preci{9}$; x$_t\preci{9}$:=[1.0,3.0]$\preci{9}$;
y$_{t-1}\preci{8}$:=0.0$\preci{8}$;
u$\preci{10}$:=0.3$\preci{8}$ *$\preci{10} $ y$_{t-1}\preci{8}$; 
v$\preci{10}$:=0.7$\preci{11}$ *$\preci{10}$ (x$_t\preci{9}$ +$\preci{10}$ x$_{t-1}\preci{9}$);
y$_t\preci{10}$:=u$\preci{10}$ +$\preci{10}$ v$\preci{10}$;
y$_{t-1}\preci{10}$:=y$_t\preci{10}$;
require_accuracy(y$_t$,10);
  \end{lstlisting}
\end{tabular}
\vspace{0.2cm}
\hrule
\end{center}
\caption{\label{figrun}
%Example of accuracy driven floating-point format determination. 
Top left: Initial  program. Top right: Annotations after analysis.
Bottom left: Forward analysis (one iteration). Bottom Right: Backward analysis (one iteration). 
} 
\end{figure}

